/*
buildscript {
    repositories {
        maven { url "https://repo.osgeo.org/repository/release/" }
        maven { url "http://jars.interlis.ch" }
        maven { url "https://plugins.gradle.org/m2/" }
        mavenCentral()
    }
    dependencies {
        classpath 'commons-net:commons-net:3.6'
    }
}
*/

gradle.startParameter.continueOnFailure = true

import ch.so.agi.gretl.tasks.*
import ch.so.agi.gretl.api.TransferSet
import ch.so.agi.gretl.api.Connector
import ch.so.agi.gretl.util.TaskUtil
import java.nio.file.Paths
import java.nio.file.Files
import java.io.FileOutputStream
import java.util.UUID

apply plugin: 'ch.so.agi.gretl'
subprojects {
    apply plugin: 'ch.so.agi.gretl'
}

apply from: "$rootDir/datasets.groovy"

defaultTasks 'exportAllData', 'uploadLogZipFileLatest'

//def pathToTempFolder = Files.createTempDirectory("agidataexport-").toFile().getAbsolutePath()

/*
String uuid = UUID.randomUUID().toString()
File tmpFolder = new File("/tmp/gretl-share","agidataexport-"+uuid);
if(!tmpFolder.exists()) {
    tmpFolder.mkdirs();
}

def pathToTempFolder = tmpFolder.getAbsolutePath()
*/

def pathToTempFolder = System.getProperty('java.io.tmpdir')

def todaysDate = new Date().format('yyyy-MM-dd')
def dbEdit = [dbUriEdit, dbUserEdit, dbPwdEdit]
def dbPub = [dbUriPub, dbUserPub, dbPwdPub]

def bucket = "ch.so.agi.geodata-dev"
if (gretlEnvironment == "test") {
    bucket = "ch.so.agi.geodata-test"
} else if (gretlEnvironment == "integration") {
    bucket = "ch.so.agi.geodata-int"
} else if (gretlEnvironment == "production") {
    bucket = "ch.so.agi.geodata"
}

datasets.each { dataset ->
    def modelNames = dataset.models
    def schema = dataset.dbschema
    def dbdatabase = dataset.database
    def datasetId = dataset.datasetId
    def isVersion3 = dataset.version3

    task "exportDataset_$datasetId"(type: Ili2pgExport) {
        database = dbdatabase
        dbschema = schema
        models = modelNames
        disableValidation = true
        logFile = file(Paths.get(pathToTempFolder, datasetId + "_" + todaysDate  + "_export.log"))
        //dataFile = file(Paths.get(pathToTempFolder, datasetId + "_" + todaysDate + ".xtf"))
        dataFile = file(Paths.get(pathToTempFolder, datasetId + ".xtf"))
        export3 = isVersion3 ? true : false
    }

    task "validateData_$datasetId"(type: IliValidator, dependsOn: "exportDataset_$datasetId") {
        //dataFiles = [Paths.get(pathToTempFolder, datasetId + "_" + todaysDate + ".xtf")]
        dataFiles = [Paths.get(pathToTempFolder, datasetId + ".xtf")]
        logFile = file(Paths.get(pathToTempFolder, datasetId + "_validation.log"))
        xtflogFile = file(Paths.get(pathToTempFolder, datasetId + "_validation.xtf"))
        failOnError = false
        if (new File("config/"+datasetId + ".toml").isFile()) configFile = "config/"+datasetId + ".toml"
    }

    task "zipDatasetXtf_$datasetId"(type: Zip, dependsOn: "validateData_$datasetId") {
        from pathToTempFolder
        include "$datasetId*.xtf"
        include "$datasetId*_export.log"
        include "$datasetId*_validation.log"
        include "$datasetId*_validation.xtf"
        archiveName datasetId + "_xtf.zip"
        destinationDir(file(pathToTempFolder))
    }

    task "uploadDatasetXtf_$datasetId"(type: S3Upload, dependsOn: "zipDatasetXtf_$datasetId") {
        accessKey = awsAccessKeyAgi
        secretKey = awsSecretAccessKeyAgi
        sourceDir = file(Paths.get(pathToTempFolder, datasetId + "_xtf.zip"))
        bucketName = bucket
        endPoint = "https://s3.eu-central-1.amazonaws.com"
        region = "eu-central-1"
        acl = "private"
        metaData = ["lastEditingDate":todaysDate]
    }
}

task exportAllData() {
    description = "Export aggregation task."
    dependsOn {
        tasks.findAll { task -> task.name.startsWith('uploadDatasetXtf_') }
    }
}

task zipLogFiles(type: Zip) {
    from pathToTempFolder
    include "*_export.log"
    include "*_validation.log"
    include "*_validation.xtf"
    archiveName  "datenvalidierung_"+todaysDate+".zip"
    destinationDir(file(pathToTempFolder))
}
zipLogFiles.mustRunAfter exportAllData

task copyLogZipFile(type: Copy, dependsOn: "zipLogFiles") {
    from pathToTempFolder
    into pathToTempFolder
    include "datenvalidierung_"+todaysDate+".zip"
    rename { String fileName ->
        fileName.replace("datenvalidierung_"+todaysDate+".zip", "datenvalidierung_latest.zip")
    }
}

task uploadLogZipFileDate(type: S3Upload, dependsOn: "copyLogZipFile") {
    accessKey = awsAccessKeyAgi
    secretKey = awsSecretAccessKeyAgi
    sourceFile = file(Paths.get(pathToTempFolder, "datenvalidierung_"+todaysDate+".zip"))
    bucketName = bucket
    endPoint = "https://s3.eu-central-1.amazonaws.com"
    region = "eu-central-1"
    acl = "public-read"
}

task uploadLogZipFileLatest(type: S3Upload, dependsOn: "uploadLogZipFileDate") {
    accessKey = awsAccessKeyAgi
    secretKey = awsSecretAccessKeyAgi
    sourceFile = file(Paths.get(pathToTempFolder, "datenvalidierung_latest.zip"))
    bucketName = bucket
    endPoint = "https://s3.eu-central-1.amazonaws.com"
    region = "eu-central-1"
    acl = "public-read"
}


/*
*/
    // TODO: only if successful?
//     task "zipDatasetXtf_$datasetId"(type: Zip, dependsOn: "exportDataset_$datasetId") {
//         from pathToTempFolder
//         include "$datasetId*.xtf"
//         include "$datasetId*_export.log"
//         include "$datasetId*_validation.log"
//         archiveName datasetId + "_xtf.zip"
//         destinationDir(file(pathToTempFolder))
//     }

//     task "importDatasetGpkg_$datasetId"(type: Ili2gpkgImport, dependsOn: "zipDatasetXtf_$datasetId") {
//         models = modelNames
//         dataFile = file(Paths.get(pathToTempFolder, datasetId + "_" + todaysDate + ".xtf"))
//         dbfile = file(Paths.get(pathToTempFolder, datasetId + "_" + todaysDate + ".gpkg"))
//         disableValidation = true
//         defaultSrsCode = 2056
//         nameByTopic = true
//         createEnumTabs = true
//         createMetaInfo = true
//         strokeArcs = true
//         coalesceJson = true
//     }

//     task "zipDatasetGpkg_$datasetId"(type: Zip, dependsOn: "importDatasetGpkg_$datasetId") {
//         from pathToTempFolder
//         include "$datasetId*.gpkg"
//         include "$datasetId*_validation.log"
//         archiveName datasetId + "_gpkg.zip"
//         destinationDir(file(pathToTempFolder))
//     }    

//     task "gpkg2Shp_$datasetId"(type: Gpkg2Shp, dependsOn: "zipDatasetGpkg_$datasetId") {
//         dataFile = file(Paths.get(pathToTempFolder, datasetId + "_" + todaysDate + ".gpkg")) 
//         outputDir = file(pathToTempFolder)
//     }

//     task "zipDatasetShp_$datasetId"(type: Zip, dependsOn: "gpkg2Shp_$datasetId") {
//         from pathToTempFolder
//         include "*.shp"
//         include "*.dbf"
//         include "*.shx"
//         include "*.prj"
//         include "$datasetId*_validation.log"
//         archiveName datasetId + "_shp.zip"
//         destinationDir(file(pathToTempFolder))

//         finalizedBy "removeShpFiles_$datasetId"
//     }      

//     task "removeShpFiles_$datasetId"(type: Delete) {
//         delete fileTree(pathToTempFolder) {
//             include '**/*.shp'
//             include '**/*.dbf'
//             include '**/*.shx'
//             include '**/*.prj'
//             include '**/*.fix'
//         }
//     }

//     task "gpkg2Dxf_$datasetId"(type: Gpkg2Dxf, dependsOn: "zipDatasetShp_$datasetId") {
//         dataFile = file(Paths.get(pathToTempFolder, datasetId + "_" + todaysDate + ".gpkg")) 
//         outputDir = file(pathToTempFolder)
//     }

//     task "zipDatasetDxf_$datasetId"(type: Zip, dependsOn: "gpkg2Dxf_$datasetId") {
//         from pathToTempFolder
//         include "*.dxf"
//         include "$datasetId*_validation.log"
//         archiveName datasetId + "_dxf.zip"
//         destinationDir(file(pathToTempFolder))

//         finalizedBy "removeDxfFiles_$datasetId"
//     }      

//     task "removeDxfFiles_$datasetId"(type: Delete) {
//         delete fileTree(pathToTempFolder) {
//             include '**/*.dxf'
//         }
//     }    

//     task "uploadDatasetXtf_$datasetId"(type: S3Upload, dependsOn: "zipDatasetDxf_$datasetId") {
//         accessKey = awsAccessKeyAgi
//         secretKey = awsSecretAccessKeyAgi
//         sourceDir = file(Paths.get(pathToTempFolder, datasetId + "_xtf.zip"))
//         bucketName = dataBucketName
//         endPoint = "https://s3.amazonaws.com/"
//         region = "eu-central-1"
//         acl = "PublicRead"
//         metaData = ["lastEditingDate":todaysDate]
//     }

//     task "uploadDatasetGpkg_$datasetId"(type: S3Upload, dependsOn: "uploadDatasetXtf_$datasetId") {
//         accessKey = awsAccessKeyAgi
//         secretKey = awsSecretAccessKeyAgi
//         sourceDir = file(Paths.get(pathToTempFolder, datasetId + "_gpkg.zip"))
//         bucketName = dataBucketName
//         endPoint = "https://s3.amazonaws.com/"
//         region = "eu-central-1"
//         acl = "PublicRead"
//         metaData = ["lastEditingDate":todaysDate]
//     }    

//     task "uploadDatasetShp_$datasetId"(type: S3Upload, dependsOn: "uploadDatasetGpkg_$datasetId") {
//         accessKey = awsAccessKeyAgi
//         secretKey = awsSecretAccessKeyAgi
//         sourceDir = file(Paths.get(pathToTempFolder, datasetId + "_shp.zip"))
//         bucketName = dataBucketName
//         endPoint = "https://s3.amazonaws.com/"
//         region = "eu-central-1"
//         acl = "PublicRead"
//         metaData = ["lastEditingDate":todaysDate]
//     }   

//     task "uploadDatasetDxf_$datasetId"(type: S3Upload, dependsOn: "uploadDatasetShp_$datasetId") {
//         accessKey = awsAccessKeyAgi
//         secretKey = awsSecretAccessKeyAgi
//         sourceDir = file(Paths.get(pathToTempFolder, datasetId + "_dxf.zip"))
//         bucketName = dataBucketName
//         endPoint = "https://s3.amazonaws.com/"
//         region = "eu-central-1"
//         acl = "PublicRead"
//         metaData = ["lastEditingDate":todaysDate]
//     }       
// }

// task exportAllData() {
//     description = "Aggregation task."
//     dependsOn {
//         tasks.findAll { task -> task.name.startsWith('uploadDatasetDxf_') }
//     }
// }


