
gradle.startParameter.continueOnFailure = true

import ch.so.agi.gretl.tasks.*
import ch.so.agi.gretl.api.TransferSet
import ch.so.agi.gretl.api.Connector
import ch.so.agi.gretl.util.TaskUtil
import java.nio.file.Paths
import java.nio.file.Files
import java.io.FileOutputStream
import java.util.UUID

apply plugin: 'ch.so.agi.gretl'
subprojects {
    apply plugin: 'ch.so.agi.gretl'
}

//defaultTasks 'exportAllData', 'uploadLogZipFileLatest'
defaultTasks 'exportAllData'

def pathToTempFolder = System.getProperty('java.io.tmpdir')

def todaysDate = new Date().format('yyyy-MM-dd')
def dbEdit = [dbUriEdit, dbUserEdit, dbPwdEdit]

def bucket = "ch.so.agi.geodata-dev"
if (gretlEnvironment == "test") {
    bucket = "ch.so.agi.geodata-test"
} else if (gretlEnvironment == "integration") {
    bucket = "ch.so.agi.geodata-int"
} else if (gretlEnvironment == "production") {
    bucket = "ch.so.agi.geodata"
}

def filenameSuffix = "ch.so.arp.nutzungsplanung_nf."
//def dataSets = ["2457"]
//def dataSets = ["2401", "2403", "2405", "2407", "2408", "2456", "2457", "2473", "2474", "2475", "2476", "2479", "2491", "2498", "2501", "2502", "2514", "2551", "2573", "2580", "2613", "2614", "2615", "2616"]
def dataSets = [2401,2402,2403,2404,2405,2406,2407,2408,2421,2422,2424,2425,2426,2427,2428,2430,2445,2455,2456,2457,2461,2463,2464,2465,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2491,2492,2493,2495,2497,2499,2500,2501,2502,2503,2511,2513,2514,2516,2517,2518,2519,2520,2523,2524,2525,2526,2527,2528,2529,2530,2532,2534,2535,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2553,2554,2555,2556,2571,2572,2573,2574,2575,2576,2578,2579,2580,2581,2582,2583,2584,2585,2586,2601,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622]

dataSets.each { dataSetId ->
    def dataSet = dataSetId.toString()
    task "exportDataset_$dataSet"(type: Ili2pgExport) {
        database = [dbUriEdit, dbUserEdit, dbPwdEdit]
        dbschema = "arp_nutzungsplanung_v1"
        models = "SO_ARP_Nutzungsplanung_Nachfuehrung_20201005"
        disableValidation = true
        logFile = file(Paths.get(pathToTempFolder, filenameSuffix+dataSet + "_" + todaysDate  + "_export.log"))
        dataFile = file(Paths.get(pathToTempFolder, filenameSuffix+dataSet + ".xtf"))
        dataset = dataSet
        failOnException = false
    }


    task "validateData_$dataSet"(type: IliValidator, dependsOn: "exportDataset_$dataSet") {
        dataFiles = [Paths.get(pathToTempFolder, filenameSuffix+dataSet + ".xtf")]
        if (findProperty('ilivalidatorModeldir')) modeldir = ilivalidatorModeldir
        logFile = file(Paths.get(pathToTempFolder, filenameSuffix+dataSet + "_validation.log"))
        xtflogFile = file(Paths.get(pathToTempFolder, filenameSuffix+dataSet + "_validation.xtf"))
        failOnError = false
    }

    task "zipDatasetXtf_$dataSet"(type: Zip, dependsOn: "validateData_$dataSet") {
        from pathToTempFolder
        include filenameSuffix+"$dataSet*.xtf"
        include filenameSuffix+"$dataSet*_export.log"
        include filenameSuffix+"$dataSet*_validation.log"
        include filenameSuffix+"$dataSet*_validation.xtf"
        archiveName filenameSuffix+dataSet + "_xtf.zip"
        destinationDir(file(pathToTempFolder))
    }

    task "uploadDatasetXtf_$dataSet"(type: S3Upload, dependsOn: "zipDatasetXtf_$dataSet") {
        accessKey = awsAccessKeyAgi
        secretKey = awsSecretAccessKeyAgi
        sourceDir = file(Paths.get(pathToTempFolder, filenameSuffix+dataSet + "_xtf.zip"))
        bucketName = bucket
        endPoint = "https://s3.eu-central-1.amazonaws.com"
        region = "eu-central-1"
        acl = "public-read"
        metaData = ["lastEditingDate":todaysDate]
    }
}

task exportAllData() {
    description = "Export aggregation task."
    dependsOn {
        tasks.findAll { task -> task.name.startsWith('uploadDatasetXtf_') }
    }
}

/*
task zipLogFiles(type: Zip) {
    from pathToTempFolder
    include "*_export.log"
    include "*_validation.log"
    include "*_validation.xtf"
    archiveName  "datenvalidierung_"+todaysDate+".zip"
    destinationDir(file(pathToTempFolder))
}
zipLogFiles.mustRunAfter exportAllData

task copyLogZipFile(type: Copy, dependsOn: "zipLogFiles") {
    from pathToTempFolder
    into pathToTempFolder
    include "datenvalidierung_"+todaysDate+".zip"
    rename { String fileName ->
        fileName.replace("datenvalidierung_"+todaysDate+".zip", "datenvalidierung_latest.zip")
    }
}

task uploadLogZipFileDate(type: S3Upload, dependsOn: "copyLogZipFile") {
    accessKey = awsAccessKeyAgi
    secretKey = awsSecretAccessKeyAgi
    sourceFile = file(Paths.get(pathToTempFolder, "datenvalidierung_"+todaysDate+".zip"))
    bucketName = bucket
    endPoint = "https://s3.eu-central-1.amazonaws.com"
    region = "eu-central-1"
    acl = "public-read"
}

task uploadLogZipFileLatest(type: S3Upload, dependsOn: "uploadLogZipFileDate") {
    accessKey = awsAccessKeyAgi
    secretKey = awsSecretAccessKeyAgi
    sourceFile = file(Paths.get(pathToTempFolder, "datenvalidierung_latest.zip"))
    bucketName = bucket
    endPoint = "https://s3.eu-central-1.amazonaws.com"
    region = "eu-central-1"
    acl = "public-read"
}
*/
