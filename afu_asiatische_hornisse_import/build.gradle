import ch.so.agi.gretl.tasks.*
import ch.so.agi.gretl.tasks.Curl.MethodType
import ch.so.agi.gretl.api.TransferSet

apply plugin: 'ch.so.agi.gretl'

description = 'GRETL-Job für den Import von Sichtungsmeldungen der Asiatischen Hornisse vom infofauna WFS'

defaultTasks 'mergeDataInDuckDb'

// DuckDB
def duckDbPath = "${buildDir}/imported.duckdb"
def duckDbUri = "jdbc:duckdb:${duckDbPath}"
def duckDbFile = file(duckDbPath)

if (duckDbFile.exists()) {
    duckDbFile.delete()
    println 'Deleted existing DuckDB database.'
}

// EditDB
def editDbHost = dbUriEdit.substring(dbUriPub.indexOf("//") + 2, dbUriEdit.lastIndexOf("/"))
def editDbDatabase = dbUriEdit.substring(dbUriEdit.lastIndexOf("/") + 1)
def editDbConnectionString = "dbname=${editDbDatabase} user=${dbUserEdit} password=${dbPwdEdit} host=${editDbHost}"

// WFS (Abfrage mit Filter auf canton = Solothurn)
def wfsUrlEncodedFirstPart = 'https://geoserver.infofauna.ch/geoserver/neovelutina/wfs?Filter=%3CFilter%3E%3CPropertyIsEqualTo%3E%3CPropertyName%3Ecanton%3C%2FPropertyName%3E%3CLiteral%3ESolothurn%3C%2FLiteral%3E%3C%2FPropertyIsEqualTo%3E%3C%2FFilter%3E&SERVICE=WFS&REQUEST=GetFeature&VERSION=2.0.0&TYPENAMES=neovelutina%3A'
def wfsUrlEncodedThirdPart = '&SRSNAME=urn%3Aogc%3Adef%3Acrs%3AEPSG%3A%3A2056&outputFormat=application%2Fjson'
def wfsUrlIndividuals = wfsUrlEncodedFirstPart + 'individuals' + wfsUrlEncodedThirdPart
def wfsUrlActiveNests = wfsUrlEncodedFirstPart + 'active_nests' + wfsUrlEncodedThirdPart
def wfsUrlUnactiveNests = wfsUrlEncodedFirstPart + 'unactive_nests' + wfsUrlEncodedThirdPart

/*
WFS-Abfrage als cURL zum Testen (mit "active_nests" als Beispiel-Layer):
curl --location 'https://geoserver.infofauna.ch/geoserver/neovelutina/wfs?SERVICE=WFS&REQUEST=GetFeature&VERSION=2.0.0&TYPENAMES=neovelutina%3Aactive_nests&SRSNAME=urn%3Aogc%3Adef%3Acrs%3AEPSG%3A%3A2056&Filter=%3CFilter%3E%3CPropertyIsEqualTo%3E%3CPropertyName%3Ecanton%3C%2FPropertyName%3E%3CLiteral%3ESolothurn%3C%2FLiteral%3E%3C%2FPropertyIsEqualTo%3E%3C%2FFilter%3E&outputFormat=application%2Fjson' \
--header 'Authorization: Basic <neovelutinaWfsUser:neovelutinaWfsPwd>'
*/

// Dateipfade
def individualsPath = "${buildDir}/infofauna_individuals.geojson"
def activeNestsPath = "${buildDir}/infofauna_active_nests.geojson"
def unactiveNestsPath = "${buildDir}/infofauna_unactive_nests.geojson"
def individualsFile = file(individualsPath)
def activeNestsFile = file(activeNestsPath)
def unactiveNestsFile = file(unactiveNestsPath)

// Sichtungsmeldungen von Hornissen herunterladen
// tbd: Abfrage auf das laufende Jahr einschränken (so dass Daten für die Dauer einer Saison nachgeführt werden)
tasks.register('downloadIndividuals', Curl) {
    serverUrl = wfsUrlIndividuals
    method = MethodType.GET
    outputFile = individualsFile
    expectedStatusCode = 200
    user = neovelutinaWfsUser
    password = neovelutinaWfsPwd
    doLast {
        println 'Hornissen-Sichtungen von InfoFauna heruntergeladen.'
    }
}

// Sichtungsmeldungen von aktiven Nestern herunterladen
// tbd: Abfrage auf das laufende Jahr einschränken (so dass Daten für die Dauer einer Saison nachgeführt werden)
tasks.register('downloadActiveNests', Curl) {
    serverUrl = wfsUrlActiveNests
    method = MethodType.GET
    outputFile = activeNestsFile
    expectedStatusCode = 200
    user = neovelutinaWfsUser
    password = neovelutinaWfsPwd
    doLast {
        println 'Bestehende Hornissen-Nester von InfoFauna heruntergeladen.'
    }
}

// Sichtungsmeldungen von zerstörten Nestern herunterladen
// tbd: Abfrage auf das laufende Jahr einschränken (so dass Daten für die Dauer einer Saison nachgeführt werden)
tasks.register('downloadUnactiveNests', Curl) {
    serverUrl = wfsUrlUnactiveNests
    method = MethodType.GET
    outputFile = unactiveNestsFile
    expectedStatusCode = 200
    user = neovelutinaWfsUser
    password = neovelutinaWfsPwd
    doLast {
        println 'Zerstörte Hornissen-Nester von InfoFauna heruntergeladen.'
    }
}

// Ausgangsdaten (heruntergeladene und bestehende) mit DuckDB einlesen
// tbd: create konsequent von insert trennen? -> Logging & Data Types
tasks.register('loadDataToDuckDb', SqlExecutor) {
    dependsOn 'downloadIndividuals', 'downloadActiveNests', 'downloadUnactiveNests'
    database = [duckDbUri]
    sqlParameters = [
        // Für die Verwendung in DuckDB-Querys Hochkommas hinzufügen und GString nach String casten
        editdb_connection_string : "'${editDbConnectionString}'".toString(),
        individuals_path: "'${individualsPath}'".toString(),
        active_nests_path: "'${activeNestsPath}'".toString(),
        unactive_nests_path: "'${unactiveNestsPath}'".toString()
        ] 
    sqlFiles = files('load_spatial.sql', 'attach_editdb.sql', 'create_duckdb_tables.sql')
}

// Unseren Datensatz mit den heruntergeladenen Daten zusammenführen
// tbd: massnahmenstatus und ..bemerkungen nach duckdb mitnehmen und zurückschreiben, weil seitens Postgres kein upsert möglich
tasks.register('mergeDataInDuckDb', SqlExecutor) {
    dependsOn 'loadDataToDuckDb'
    database = [duckDbUri]
    sqlFiles = files('upsert_individuals.sql')
}

// Zusammengeführte Daten in die Edit-DB zurückkopieren
// tbd: das ist ein vorläufiger Entwicklungsstand; Ziel: merge mit bestehenden Daten anstatt overwrite
tasks.register('copyToEditDb', Db2Db) {
    dependsOn 'mergeDataInDuckDb'
    sourceDb = [duckDbUri]
    targetDb = [dbUriEdit, dbUserEdit, dbPwdEdit]
    transferSets = [
        new TransferSet('select_duckdb_individuals.sql', 'afu_asiatische_hornisse_v2.asia_hornisse_sichtung', true, (String[])['geometrie:wkt:2056']),
        // new TransferSet('select_duckdb_nests.sql', 'afu_asiatische_hornisse_v2.asia_hornisse_nest', true, (String[])['geometrie:wkt:2056'])
    ];
}