import ch.so.agi.gretl.tasks.*
import ch.so.agi.gretl.tasks.Curl.MethodType
import ch.so.agi.gretl.api.TransferSet

apply plugin: 'ch.so.agi.gretl'

description = 'GRETL-Job für den Import von Sichtungsmeldungen der Asiatischen Hornisse vom infofauna WFS'

defaultTasks 'copyResultsToEditDb'

// DuckDB
def duckDbPath = "${buildDir}/imported.duckdb"
def duckDbUri = "jdbc:duckdb:${duckDbPath}"
def duckDbFile = file(duckDbPath)

if (duckDbFile.exists()) {
    duckDbFile.delete()
    println 'Deleted existing DuckDB database.'
}

// EditDB
def editDbHost = dbUriEdit.substring(dbUriPub.indexOf("//") + 2, dbUriEdit.lastIndexOf("/"))
def editDbDatabase = dbUriEdit.substring(dbUriEdit.lastIndexOf("/") + 1)
def editDbConnectionString = "dbname=${editDbDatabase} user=${dbUserEdit} password=${dbPwdEdit} host=${editDbHost}"

// WFS (Abfrage mit Filter auf canton = Solothurn)
def wfsUrlEncodedFirstPart = 'https://geoserver.infofauna.ch/geoserver/neovelutina/wfs?Filter=%3CFilter%3E%3CPropertyIsEqualTo%3E%3CPropertyName%3Ecanton%3C%2FPropertyName%3E%3CLiteral%3ESolothurn%3C%2FLiteral%3E%3C%2FPropertyIsEqualTo%3E%3C%2FFilter%3E&SERVICE=WFS&REQUEST=GetFeature&VERSION=2.0.0&TYPENAMES=neovelutina%3A'
def wfsUrlEncodedThirdPart = '&SRSNAME=urn%3Aogc%3Adef%3Acrs%3AEPSG%3A%3A2056&outputFormat=application%2Fjson'
def wfsUrlIndividuals = wfsUrlEncodedFirstPart + 'individuals' + wfsUrlEncodedThirdPart
def wfsUrlActiveNests = wfsUrlEncodedFirstPart + 'active_nests' + wfsUrlEncodedThirdPart
def wfsUrlUnactiveNests = wfsUrlEncodedFirstPart + 'unactive_nests' + wfsUrlEncodedThirdPart

/*
WFS-Abfrage als cURL zum Testen (mit "active_nests" als Beispiel-Layer):
curl --location 'https://geoserver.infofauna.ch/geoserver/neovelutina/wfs?SERVICE=WFS&REQUEST=GetFeature&VERSION=2.0.0&TYPENAMES=neovelutina%3Aactive_nests&SRSNAME=urn%3Aogc%3Adef%3Acrs%3AEPSG%3A%3A2056&Filter=%3CFilter%3E%3CPropertyIsEqualTo%3E%3CPropertyName%3Ecanton%3C%2FPropertyName%3E%3CLiteral%3ESolothurn%3C%2FLiteral%3E%3C%2FPropertyIsEqualTo%3E%3C%2FFilter%3E&outputFormat=application%2Fjson' \
--header 'Authorization: Basic <neovelutinaWfsUser:neovelutinaWfsPwd>'
*/

// Dateipfade
def individualsPath = "${buildDir}/infofauna_individuals.geojson"
def activeNestsPath = "${buildDir}/infofauna_active_nests.geojson"
def unactiveNestsPath = "${buildDir}/infofauna_unactive_nests.geojson"
def individualsFile = file(individualsPath)
def activeNestsFile = file(activeNestsPath)
def unactiveNestsFile = file(unactiveNestsPath)

// Sichtungsmeldungen von Hornissen herunterladen
// tbd: Abfrage auf das laufende Jahr einschränken (so dass Daten für die Dauer einer Saison nachgeführt werden)
tasks.register('downloadIndividuals', Curl) {
    serverUrl = wfsUrlIndividuals
    method = MethodType.GET
    outputFile = individualsFile
    expectedStatusCode = 200
    user = neovelutinaWfsUser
    password = neovelutinaWfsPwd
    doLast {
        println 'Hornissen-Sichtungen von InfoFauna heruntergeladen.'
    }
}

// Sichtungsmeldungen von aktiven Nestern herunterladen
// tbd: Abfrage auf das laufende Jahr einschränken (so dass Daten für die Dauer einer Saison nachgeführt werden)
tasks.register('downloadActiveNests', Curl) {
    serverUrl = wfsUrlActiveNests
    method = MethodType.GET
    outputFile = activeNestsFile
    expectedStatusCode = 200
    user = neovelutinaWfsUser
    password = neovelutinaWfsPwd
    doLast {
        println 'Bestehende Hornissen-Nester von InfoFauna heruntergeladen.'
    }
}

// Sichtungsmeldungen von zerstörten Nestern herunterladen
// tbd: Abfrage auf das laufende Jahr einschränken (so dass Daten für die Dauer einer Saison nachgeführt werden)
tasks.register('downloadUnactiveNests', Curl) {
    serverUrl = wfsUrlUnactiveNests
    method = MethodType.GET
    outputFile = unactiveNestsFile
    expectedStatusCode = 200
    user = neovelutinaWfsUser
    password = neovelutinaWfsPwd
    doLast {
        println 'Zerstörte Hornissen-Nester von InfoFauna heruntergeladen.'
    }
}

// Ausgangsdaten (heruntergeladene und bestehende) mit DuckDB einlesen
// tbd: create konsequent von insert trennen -> Logging & Data Types
tasks.register('loadDataToDuckDb', SqlExecutor) {
    dependsOn 'downloadIndividuals', 'downloadActiveNests', 'downloadUnactiveNests'
    database = [duckDbUri]
    sqlParameters = [
        // Für die Verwendung in DuckDB-Querys Hochkommas hinzufügen und GString nach String casten
        editdb_connection_string : "'${editDbConnectionString}'".toString(),
        individuals_path: "'${individualsPath}'".toString(),
        active_nests_path: "'${activeNestsPath}'".toString(),
        unactive_nests_path: "'${unactiveNestsPath}'".toString()
    ]
    sqlFiles = files(
        'load_spatial.sql',
        'attach_editdb_read.sql',
        'create_duckdb_table_afu_individuals.sql',
        'create_duckdb_table_afu_nests.sql',
        'create_duckdb_table_infofauna_individuals.sql',
        'create_duckdb_table_infofauna_nests.sql'
    )
}

// Unseren Datensatz mit den heruntergeladenen Daten zusammenführen
// tbd: t_id und t_ili_id mitnehmen und rückkopieren oder nicht? (aktuell: nicht)
// tbd: check lat lon constraint failures

// Upsert ist nicht mit attached PostgreSQL DB möglich wegen einer Limitation
// von DuckDB (https://github.com/duckdb/duckdb-postgres/issues/189). Darum
// innerhalb duckDB mit anschliessendem Db2DB nach Edit-DB.
tasks.register('upsertIndividualsInsideDuckDb', SqlExecutor) {
    dependsOn 'loadDataToDuckDb'
    database = [duckDbUri]
    sqlParameters = [
        // Für die Verwendung in DuckDB-Querys Hochkommas hinzufügen und GString nach String casten
        editdb_connection_string : "'${editDbConnectionString}'".toString()
    ] 
    sqlFiles = files(
        'load_spatial.sql',
        'attach_editdb_read.sql',
        'upsert_duckdb_individuals.sql'
    )
}

tasks.register('upsertNestsInsideDuckDb', SqlExecutor) {
    dependsOn 'loadDataToDuckDb'
    database = [duckDbUri]
    sqlParameters = [
        // Für die Verwendung in DuckDB-Querys Hochkommas hinzufügen und GString nach String casten
        editdb_connection_string : "'${editDbConnectionString}'".toString()
    ] 
    sqlFiles = files(
        'load_spatial.sql',
        'attach_editdb_read.sql',
        'upsert_duckdb_nests.sql'
    )
}

// Zusammengeführte Daten in die Edit-DB zurückkopieren (überschreibend).
// Achtung: Db2Db unterstützt kein LOAD SPATIAL Query, somit stehen keine räumlichen
// Funktionen zur Verfügung. Und um Geometrien zu kopieren, ist der Umweg über WKT nötig.
tasks.register('copyResultsToEditDb', Db2Db) {
    dependsOn 'upsertIndividualsInsideDuckDb', 'upsertNestsInsideDuckDb'
    sourceDb = [duckDbUri]
    targetDb = [dbUriEdit, dbUserEdit, dbPwdEdit]
    transferSets = [
        new TransferSet('select_duckdb_individuals.sql', 'afu_asiatische_hornisse_v2.asia_hornisse_sichtung', true, (String[])['geometrie:wkt:2056']),
        new TransferSet('select_duckdb_nests.sql', 'afu_asiatische_hornisse_v2.asia_hornisse_nest', true, (String[])['geometrie:wkt:2056'])
    ];
}